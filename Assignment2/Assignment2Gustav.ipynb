{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Graphs 2021 - Assignment 2\n",
    "Welcome to our exercise 2! Hope you enyoy the read, see you on the other side ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import wordcloud\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "import re\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "import powerlaw\n",
    "from fa2 import ForceAtlas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Styles and data. If this cell block is not already hidden/minimized, feel free to do so, as it only contains the styles for the plots.\n",
    "DTU_COLORS_HEX = dict(\n",
    "    dtured      = \"#990000\",\n",
    "    blue        = \"#2F3EEA\",\n",
    "    brightgreen = \"#1FD082\",\n",
    "    navyblue    = \"#030F4F\",\n",
    "    yellow      = \"#F6D04D\",\n",
    "    orange      = \"#FC7634\",\n",
    "    pink        = \"#F7BBB1\",\n",
    "    grey        = \"#DADADA\",\n",
    "    red         = \"#E83F48\",\n",
    "    green       = \"#008835\",\n",
    "    purple      = \"#79238E\"\n",
    ")\n",
    "\n",
    "sns.set()\n",
    "#sns.set_palette(\"Spectral\")\n",
    "\n",
    "cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "cmap = ListedColormap(DTU_COLORS_HEX.values())\n",
    "\n",
    "plt.rcParams[\"image.cmap\"] = \"Set1\"\n",
    "\n",
    "big_plot = dict(\n",
    "    figsize     = (20, 20),\n",
    ")\n",
    "\n",
    "default = dict(\n",
    "    figsize     = (20, 5),\n",
    ")\n",
    "\n",
    "ListedColormap(DTU_COLORS_HEX.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Building the network\n",
    "This part will explain the strategy and regular expressions for each task: Downloading, finding gender and race and extracting links to characters.\n",
    "\n",
    "## Strategy for extracting hyperlinks from the wiki-pages\n",
    "### Downloading\n",
    "First, we download the three wiki pages containing the ally, enemy and boss characters. From this, we use a regular expression to find all character names.\n",
    "### Regex\n",
    "First, the boss page has an edge case, where the boss's location is included in the same format as the name of the boss. To remove this, we use ```{{Small.*?''}}``` to find the locations and remove them.\n",
    "\n",
    "When these are removed we use ```({{Term\\|BotW\\|(.*?)\\|link[\\|]*.*?}})``` to find the names of each character. All names follow the same convention of `{{Term|BotW|NAME OF CHARACTER|link|MAYBE MORE STUFF}}` occasionally followed by further information. This is solved by using `*.*?`, which lazily finds additional terms. `*?` makes sure that this is only done until the following character `}` is found.\n",
    "\n",
    "## Strategy for finding gender and race\n",
    "### Downloading\n",
    "From the character names found in the previous step, we now download the content of each character using the API.\n",
    "\n",
    "From this, we do two regular expressions to find race and gender.\n",
    "\n",
    "### Regex\n",
    "* Race: `{{Infobox.*?\\|(?:race=|species=)\\s(.*?)(?:\\n|<ref).*?(?:\\|gender=\\s`**(.*?)**`(?:\\n|<ref)|).*?}}`. \n",
    "\n",
    "For each character page, we use this expression. All meta information, including the races, is contained in an Infobox, explaining the beginning and end of the expressions. We use capture groups to allow for or-statements signified by `(|)` the |-symbol, as this cannot be written outside a capture group. Whether the keyword is `race` or `species` is not essential. `(?:)` is used to make the capture group not return anything. The same convention is followed throughout the expression to include different types of definitions. Lastly, the small group `(.*?)` is used to find the character's race. Characters might have multiple races, in which case only the first is chosen.\n",
    "\n",
    "* Gender: `.*(Male|Female).*`. \n",
    "\n",
    "Genders are only included inside of infoboxes, which we found in the previous expression. Using the extraction of the prior expression, this expression is now used. This expression is only run for these infoboxes. The expression is only used on these, searching for males or females, where the `|`-symbol signifies an or-statement. For genders outside of male or female, the gender `unknown` is assigned to the character. \n",
    "\n",
    "## Strategy for extracting links to characters\n",
    "Using the content of each character page, we now find the links. The code for finding the links can be in the following code block. \n",
    "\n",
    "### Regex\n",
    "The links we observed came in two flavours: curly and square brackets, following slightly different conventions.\n",
    "\n",
    "* Square brackets: `\\[\\[`**(.*?)**`(?:\\|.*?|\\#.*?)?\\]\\]`\n",
    "\n",
    "All square bracket links are enclosed in double square brackets (`[[.*]]`). From inside, we find the actual link; the bold-fonted expression `(.*?)`. This lazily reads all information until it encounters a vertical line (`|`) or the closing square bracket `]`.\n",
    "\n",
    "* Curly brackets: `{{(?:Term|Plural)\\|.*?\\|`**(.*?)**`(?:\\|.*?)?}}`\n",
    "\n",
    "All curly bracket links are also using double curly brackets (`{{.*}}`). Indifference to the square brackets, this is followed by either \"Term\" or \"Plural\", found and filtered out using `(?:Term|Plural)`. Otherwise, the links follow the same convention as the square brackets, and the capture group highlighted in bold is the expression used to find the link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Network visualization and basic stats\n",
    "\n",
    "## Preliminary\n",
    "The code below finds the character links using the regex expressions previosly discussed, and extracts the giant component using networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Characters.csv\")\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "for _, (name, role, race, gender) in df.iterrows():\n",
    "    G.add_node(name, role=role, race=race, gender=gender)\n",
    "\n",
    "# Expressions\n",
    "brackets_pattern = r'\\[\\[(.*?)(?:\\|.*?|\\#.*?)?\\]\\]'\n",
    "curly_pattern =    r'{{(?:Term|Plural)\\|.*?\\|(.*?)(?:\\|.*?)?}}'\n",
    "\n",
    "for name in df.Name:\n",
    "    description = open(f\"././Characters/{name}.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "    links = []\n",
    "    links.extend(re.findall(brackets_pattern, description))\n",
    "    links.extend(re.findall(curly_pattern, description))\n",
    "    for link in links:\n",
    "        if link in df.Name.values and link != name:\n",
    "            G.add_edge(name, link)\n",
    "\n",
    "GCC = G.subgraph(max(nx.weakly_connected_components(G), key=len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing stats for the giant component of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of nodes in network: \\t {GCC.number_of_nodes()}\")\n",
    "print(f\"Number of linksðŸ”—: \\t\\t {GCC.number_of_edges()}\")\n",
    "\n",
    "sorted_in  = sorted(GCC.nodes, key=GCC.in_degree, reverse=True)\n",
    "sorted_out = sorted(GCC.nodes, key=GCC.out_degree, reverse=True)\n",
    "sorted_tot = sorted(GCC.nodes, key=GCC.degree, reverse=True)\n",
    "\n",
    "print(f\"Most connected in:\\t {sorted_in[0]}\\n\\tinðŸ‘ˆ:\\t{GCC.in_degree(sorted_in[0])}\\n\\toutðŸ‘‰:\\t{GCC.out_degree(sorted_in[0])}\")\n",
    "print(f\"Most connected out:\\t {sorted_out[0]}\\n\\tinðŸ‘ˆ:\\t{GCC.in_degree(sorted_out[0])}\\n\\toutðŸ‘‰:\\t{GCC.out_degree(sorted_out[0])}\")\n",
    "print(f\"Most connected overall:\\t {sorted_tot[0]}\\n\\tinðŸ‘ˆ:\\t{GCC.in_degree(sorted_tot[0])}\\n\\toutðŸ‘‰:\\t{GCC.out_degree(sorted_tot[0])}\")\n",
    "\n",
    "allies = df[df.Role==\"Ally\"].Name.to_list()\n",
    "enemies = df[df.Role.isin([\"Enemy\", \"Boss\"])].Name.to_list()\n",
    "\n",
    "sorted_allies = list(filter(lambda x: x in allies, sorted_tot))\n",
    "sorted_enemies = list(filter(lambda x: x in enemies, sorted_tot))\n",
    "\n",
    "# Seems a bit long to show same result as above for all allies and enemies,\n",
    "# so here we only look at total degree\n",
    "print(\"Most connected allies âœŒ (name, in, out) \")\n",
    "print(\"\\t\"+\"\\n\\t\".join([\", \".join([str(x), str(GCC.in_degree(x)), str(GCC.out_degree(x))]) for x in sorted_allies[:5]]))\n",
    "print(\"Most connected enemies ðŸ˜ˆ (name, in, out) \")\n",
    "print(\"\\t\"+\"\\n\\t\".join([\", \".join([str(x), str(GCC.in_degree(x)), str(GCC.out_degree(x))]) for x in sorted_enemies[:5]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats\n",
    "The number of nodes and links can be seen above.\n",
    "\n",
    "## Most connected character.\n",
    "Link being the most connected character is not surprising, as he is the franchise's main character. However, it surprised us a bit that he is not the most out-wards connected character, and Traysi is instead. After reading about her, it makes sense, as she has a quest where Link has to talk to different people, and all of these people are all linked on her page, which is of course not mentioned on Links page as it is quite a small quest, but all of it is mentioned on her page, as that is most of her role in the game.\n",
    "\n",
    "## Most(ly) connected allies and enemies\n",
    "It is more or less what was expected for the allies, as all the characters that appear in the top 5 are main characters. For the enemies, it's a bit confusing, as all other characters than Calamity Ganon are not characters but generic enemies. However, it does make sense that they are in the top 5, as they are commonly occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the in- and out-degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (in_ax, out_ax) = plt.subplots(1, 2, **default)\n",
    "fig.suptitle(\"Degree distribution\")\n",
    "\n",
    "in_degrees = dict(GCC.in_degree()).values()\n",
    "out_degrees = dict(GCC.out_degree()).values()\n",
    "\n",
    "counts, bins, bars = in_ax.hist(in_degrees, bins=100)\n",
    "in_ax.set_title(\"In degree distribution\")\n",
    "in_ax.set_xlabel(\"Degrees\")\n",
    "in_ax.set_ylabel(\"Count\")\n",
    "in_ax.set_xticks(np.arange(0, 420, step=20))\n",
    "\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= max(col)\n",
    "col *= 10\n",
    "\n",
    "\n",
    "for c, p in zip(col, bars):\n",
    "  plt.setp(p, 'facecolor', cmap(c))\n",
    "\n",
    "for c, b in zip(counts, bins):\n",
    "    if c > 0: in_ax.text(b, c+5, f\"{c:.0f}\")\n",
    "      \n",
    "      \n",
    "counts, bins, bars = out_ax.hist(out_degrees)\n",
    "out_ax.set_title(\"Out degree distribution\")\n",
    "out_ax.set_xlabel(\"Degrees\")\n",
    "out_ax.set_ylabel(\"Count\")\n",
    "\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= max(col)\n",
    "\n",
    "\n",
    "for c, p in zip(col, bars):\n",
    "  plt.setp(p, 'facecolor', cmap(c))\n",
    "\n",
    "for c, b in zip(counts, bins):\n",
    "  if c > 0: out_ax.text(b+.4, c+2, f\"{c:.0f}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on the in- and out-degree distributions \n",
    "\n",
    "The first apparent difference that one can see is that for the in-degree distribution, there is one notable outlier that skews the plot a lot (Link). To fully explain the differences, it's necessary to use more bins in the in-degree distribution plot.\n",
    "\n",
    "Once that is done, it is clear that most characters have an in-degree very small (between 0 and 5). That does make sense because most secondary characters do not have external references in other character wikis, only the main characters. The Link outlier makes sense, too, as are the main characters, and most wiki pages reference him.\n",
    "\n",
    "On the other hand, the out-degree is more spread, without outliers. Most characters make a few references in their wiki to other characters, and only a tiny percentage of them have a lot of out-degree links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponent of the degree distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree = [d for n, d in GCC.in_degree() if d > 0]\n",
    "out_degree = [d for n, d in GCC.out_degree() if d > 0]\n",
    "\n",
    "results_in = powerlaw.Fit(in_degree, xmin=1)\n",
    "results_out = powerlaw.Fit(out_degree, xmin=1)\n",
    "\n",
    "print(f\"Exponent of the degree distribution for the in-degree:\\t {results_in.alpha}\")\n",
    "print(f\"Exponent of the degree distribution for the out-degree:\\t {results_out.alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on the exponent of the degree distribution\n",
    "\n",
    "As shown in the previous calculations with the `powerlaw` package, the degree exponent for the in-degree distribution is slightly higher than `2.09`, which would fit in the *Scale-Free Regime*. That means that this degree distribution follows a scale-free distribution.\n",
    "\n",
    "Meanwhile, the degree exponent for the out-degree distribution is under `2.0`, which belongs to the *Anomalous Regime*. That means that the number of links connected to the largest hub grows faster than the size of the network. So eventually, with more nodes, it would become a scale-free distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree distribution and random network comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(GCC.nodes)\n",
    "prob_edge = (2 * len(GCC.edges)/num_nodes)/(num_nodes - 1)\n",
    "\n",
    "random_g = nx.generators.random_graphs.erdos_renyi_graph(n=num_nodes, p=prob_edge)\n",
    "print(f\"Random network generated with {len(random_g.nodes)} nodes and {len(random_g.edges)} links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (rand_ax, gcc_ax) = plt.subplots(1, 2, **default)\n",
    "random_degrees = dict(random_g.degree()).values()\n",
    "GCC_degrees = dict(GCC.degree()).values()\n",
    "\n",
    "counts, bins, bars = rand_ax.hist(random_degrees)\n",
    "rand_ax.set_title(\"Degree distribution in random network\")\n",
    "rand_ax.set_xlabel(\"Degrees\")\n",
    "rand_ax.set_ylabel(\"Count\")\n",
    "\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= max(col)\n",
    "\n",
    "\n",
    "for c, p in zip(col, bars):\n",
    "  plt.setp(p, 'facecolor', cmap(c))\n",
    "\n",
    "for c, b in zip(counts, bins):\n",
    "    if c > 0: rand_ax.text(b+.4, c+5, f\"{c:.0f}\")\n",
    "\n",
    "counts, bins, bars = gcc_ax.hist(GCC_degrees, bins=100)\n",
    "gcc_ax.set_title(\"Degree distribution in the BotW GCC\")\n",
    "gcc_ax.set_xlabel(\"Degrees\")\n",
    "gcc_ax.set_ylabel(\"Count\")\n",
    "\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= max(col)\n",
    "col *= 10\n",
    "\n",
    "\n",
    "for c, p in zip(col, bars):\n",
    "  plt.setp(p, 'facecolor', cmap(c))\n",
    "\n",
    "for c, b in zip(counts, bins):\n",
    "    if c > 0: gcc_ax.text(b, c+2, f\"{c:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on degree distribution and random network comparison\n",
    "\n",
    "As expected, the degree distribution in a random network follows a Poisson distribution. In contrast, the degree distribution of the BotW GCC follows a scale-free distribution, or a least is much closer to that distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.b\n",
    "First, we create the placement of nodes using the forceatlas2 algorithm. Then we visualize and reflect on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=4.0,\n",
    "                        strongGravityMode=True,\n",
    "                        gravity=0.1,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(GCC, pos=None, iterations=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "```\n",
    "  Color of the node   -> Role\n",
    "  Size of the node    -> Degree\n",
    "  Color of the border -> Race\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(**big_plot)\n",
    "plt.plot()\n",
    "\n",
    "color_role = dict(\n",
    "  Ally  = DTU_COLORS_HEX[\"dtured\"],\n",
    "  Enemy = DTU_COLORS_HEX[\"blue\"],\n",
    "  Boss  = DTU_COLORS_HEX[\"brightgreen\"]\n",
    ")\n",
    "\n",
    "color_race = {\n",
    "  \"Hylian\"         : DTU_COLORS_HEX[\"blue\"], \n",
    "  \"Horse\"          : DTU_COLORS_HEX[\"brightgreen\"],\n",
    "  \"Goron\"          : DTU_COLORS_HEX[\"navyblue\"],\n",
    "  \"Human\"          : DTU_COLORS_HEX[\"yellow\"],\n",
    "  \"Dragon\"         : DTU_COLORS_HEX[\"orange\"], \n",
    "  \"Rito\"           : DTU_COLORS_HEX[\"pink\"],\n",
    "  \"Unknown\"        : DTU_COLORS_HEX[\"grey\"],\n",
    "  \"Korok\"          : DTU_COLORS_HEX[\"red\"],\n",
    "  \"Zora\"           : DTU_COLORS_HEX[\"green\"],\n",
    "  \"Sheikah\"        : DTU_COLORS_HEX[\"purple\"],\n",
    "  \"Gerudo\"         : DTU_COLORS_HEX[\"blue\"],\n",
    "  \"Great Fairy\"    : DTU_COLORS_HEX[\"brightgreen\"],\n",
    "  \"Ancient Orb\"    : DTU_COLORS_HEX[\"navyblue\"],\n",
    "  \"Sand Seal\"      : DTU_COLORS_HEX[\"yellow\"],\n",
    "  \"Dog\"            : DTU_COLORS_HEX[\"orange\"],\n",
    "  \"Bokoblin\"       : DTU_COLORS_HEX[\"dtured\"],  \n",
    "  \"Lizalfos\"       : DTU_COLORS_HEX[\"yellow\"],\n",
    "  \"Moblin\"         : DTU_COLORS_HEX[\"orange\"],\n",
    "  \"Wizzrobe\"       : DTU_COLORS_HEX[\"pink\"],\n",
    "  \"Lynel\"          : DTU_COLORS_HEX[\"grey\"],\n",
    "  \"Chuchu\"         : DTU_COLORS_HEX[\"red\"],\n",
    "  \"Guardian\"       : DTU_COLORS_HEX[\"green\"],\n",
    "  \"Keese\"          : DTU_COLORS_HEX[\"purple\"],\n",
    "  \"Octorok\"        : DTU_COLORS_HEX[\"navyblue\"],\n",
    "  \"Talus\"          : DTU_COLORS_HEX[\"yellow\"],\n",
    "  \"Hinox\"          : DTU_COLORS_HEX[\"orange\"],\n",
    "  \"Molduga\"        : DTU_COLORS_HEX[\"pink\"]\n",
    "}\n",
    "\n",
    " \n",
    "\n",
    "scale = 50\n",
    "\n",
    "node_info = list()\n",
    "labels = dict()\n",
    "\n",
    "for node, node_data in list(GCC.nodes(data=True)):\n",
    "  \n",
    "  color = color_role[node_data[\"role\"]]\n",
    "  size = GCC.degree(node)*scale\n",
    "  border_color = color_race[node_data[\"race\"]]\n",
    "  \n",
    "  \n",
    "  node_info.append((node, color, size, border_color))\n",
    "\n",
    "nx.draw_networkx_nodes(GCC,\n",
    "                       positions,\n",
    "                       edgecolors  = [x[3] for x in node_info],\n",
    "                       linewidths  = 3,\n",
    "                       node_color  = [x[1] for x in node_info],\n",
    "                       node_size   = [x[2] for x in node_info]  ,\n",
    "                       alpha       = 0.4\n",
    "                      )\n",
    "\n",
    "nx.draw_networkx_edges(GCC,\n",
    "                       positions,\n",
    "                       edge_color  = \"black\",\n",
    "                       arrowstyle  = \"-\",\n",
    "                       alpha       = 0.1\n",
    "                      )  \n",
    "plt.axis(\"off\")\n",
    "plt.title(\"BotW graph where:\\n \\\n",
    "          The color of the node indicates the role\\n \\\n",
    "          The color of the border indicates the race\\n \\\n",
    "          The size indicates the degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on visualization\n",
    "\n",
    "As expected, the biggest node is 'Link', as is the one most connected. Other significant nodes that can be seen are 'Calamity Ganon' or 'Princess Zelda', also central characters.\n",
    "\n",
    "Another thing that can be seen quite clearly with the graph is that bosses and enemies tend to be referenced together, as they have many links between them. The same happens with the allies. Of course, some nodes that are allies are connected to enemies.\n",
    "\n",
    "About the connections between nodes, we can see that most nodes are connected to other nodes of the same race, which makes sense, as, in the game, the character of the same race are in the same areas, so they make references to other characters of the same place.\n",
    "\n",
    "\n",
    "*Side note: The colouring by race is not perfect, as some colours are repeating. This is because we wanted to do something visually nice and use the [DTU-colorscheme](https://www.designguide.dtu.dk/standard-design-basics#standard-user-color-banner). The colours we feel, do still help in the general visualization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Wordclouds\n",
    "This part will explain the process of constructing the wordclouds. The construction of the wordclouds includes three steps: fetching the descriptions, preprocessing the descriptions and calculating TF and IDF.\n",
    "\n",
    "## Fetching the descriptions\n",
    "This is mostly solved in the same way as previous. For each character name, the description is fetched using the API and stored in a .txt file.\n",
    "\n",
    "## Preprocessing the descriptions\n",
    "In this step, first, the headers are removed denoted with the `=`-symbol.\n",
    "\n",
    "Because it seemed more straightforward to use regex to remove punctuation, this is done as well.\n",
    "\n",
    "Then some of the muscle power ðŸ’ª from nltk is used to remove all English stopwords, the names of all known characters, and the words are lemmatized.\n",
    "\n",
    "## TF and IDF\n",
    "Following the formulas of TF and IDF, the values are calculated for each term and stored in a pd.DataFrame. This is done to use vectorization to speed up the process a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading Descriptions for each character\n",
    "\n",
    "df = pd.read_csv(\"Characters.csv\")\n",
    "txt_path = \"./Descriptions\"\n",
    "\n",
    "baseurl = \"https://zelda.fandom.com/api.php?\"\n",
    "content = \"action=query&titles={name}&prop=extracts&exlimit=1&explaintext&format=json&indexpageids=true\"\n",
    "\n",
    "def download_txt(row):\n",
    "    name = row.Name.replace(\" \", \"_\")\n",
    "    quoted_name = urllib.parse.quote_plus(name)\n",
    "    url = baseurl + content.format(name=quoted_name)\n",
    "\n",
    "    data = requests.get(url).json()\n",
    "    extract = list(data[\"query\"][\"pages\"].values())[0][\"extract\"]\n",
    "    \n",
    "    with open(f\"{txt_path}/{row.Name}.txt\", \"w\") as f:\n",
    "        f.write(str(extract.encode(encoding=\"utf-8\", errors=\"xmlcharrefreplace\")))\n",
    "\n",
    "download = False\n",
    "if download: df.apply(download_txt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing each character description removing, tokenizing and lemmatizing.\n",
    "\n",
    "raw_path = \"Descriptions/\"\n",
    "processed_path = \"ProcessedDescriptions/\"\n",
    "names = df.Name.values\n",
    "\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess_zelda_descriptions(row):\n",
    "    filename = row.Name + \".txt\"\n",
    "    txt = open(raw_path + filename, \"r\").read()[2:-1]\n",
    "    txt.encode('latin1').decode(\"utf-8\")\n",
    "\n",
    "    txt = re.sub(\"=+.*?=+\", \" \", txt) # Removes header\n",
    "    txt = re.sub(r\"\\\\n|\\.|,|\\\\\", \" \", txt) # Remove signs\n",
    "\n",
    "    token = tokenizer.tokenize(txt)\n",
    "    all_words = [x.strip().lower() for x in token]\n",
    "\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\") # Stopwords\n",
    "    stop_words += [\".\", \",\", \"'\", \"\\\"\", \"/\", \")\", \"(\", \"*\", \"\\*\"] # Some signs were not removed through regex\n",
    "    stop_words += [x.lower() for x in names] # Names\n",
    "\n",
    "    filtered = [w for w in all_words if w not in stop_words]\n",
    "\n",
    "    lemmatized = [wnl.lemmatize(w) for w in filtered]\n",
    "\n",
    "    with open(processed_path + filename, \"w\") as f:\n",
    "        f.write(\" \".join(lemmatized))\n",
    "\n",
    "process = True\n",
    "if process: df.apply(preprocess_zelda_descriptions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TF, IDF and TF-IDF for each lemmatized word in each race\n",
    "\n",
    "races = {}\n",
    "\n",
    "def TF(term, text: nltk.Text) -> float:\n",
    "    return text.count(term) / len(text)\n",
    "\n",
    "def IDF(terms, wlist: nltk.corpus.PlaintextCorpusReader) -> pd.DataFrame:\n",
    "    counts = [[term, 0] for term in terms]\n",
    "\n",
    "    for file_id in wlist.fileids():\n",
    "        txt = wlist.raw(file_id).split(\" \")\n",
    "        for row in counts:\n",
    "            if row[0] in txt: row[1] += 1\n",
    "\n",
    "    i_df = pd.DataFrame(counts, columns=[\"Term\", \"IDF\"])\n",
    "    i_df[\"IDF\"] = np.log2(len(wlist.fileids())/i_df[\"IDF\"])\n",
    "\n",
    "    return i_df\n",
    "\n",
    "for race in tqdm(df.Race.unique().tolist()):\n",
    "    filenames = [name + \".txt\" for name in df[df.Race==race].Name]\n",
    "    wlist = nltk.corpus.PlaintextCorpusReader(processed_path, filenames)\n",
    "\n",
    "    terms = set(wlist.words())\n",
    "\n",
    "    text = nltk.Text(wlist.words())\n",
    "    tfs = [[term, TF(term, text)] for term in terms]\n",
    "    tdf = pd.DataFrame(tfs, columns=[\"Term\", \"TF\"])\n",
    "\n",
    "    idf = IDF(terms, wlist)\n",
    "    \n",
    "    races[race] = {}\n",
    "    races[race][\"pd\"] = pd.merge(tdf, idf, on=\"Term\")\n",
    "    races[race][\"wlist\"] = wlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating wordclouds\n",
    "\n",
    "race_order  = [\n",
    "    \"Hylian\", \n",
    "    \"Zora\", \n",
    "    \"Goron\", \n",
    "    \"Gerudo\", \n",
    "    \"Rito\"\n",
    "]\n",
    "\n",
    "rows = 2\n",
    "cols = 3\n",
    "cloud_size = 1000\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(24, 16))\n",
    "for ax, race in tqdm(zip(axs.flatten(), race_order), total=5):\n",
    "    race_df = races[race][\"pd\"]\n",
    "    \n",
    "    # Repeat each word int(IDF)-number of times\n",
    "    #words = [[x.Term] * int(x.IDF) for x in race_df.itertuples()] \n",
    "    words = [[x.Term] for x in race_df.itertuples()] \n",
    "    \n",
    "    # Flatten the list of lists\n",
    "    flattened_words = [item for subl in words for item in subl] \n",
    "\n",
    "    # Join all the words\n",
    "    joined_words = \" \".join(flattened_words)\n",
    "\n",
    "    mask = plt.imread(f\"./images/{race}.jpg\")\n",
    "\n",
    "    wcloud = wordcloud.WordCloud(\n",
    "        background_color=\"white\",\n",
    "        width=cloud_size,\n",
    "        height=cloud_size,\n",
    "        mask=mask,\n",
    "        font_path=\"./NeoSans Black.otf\",\n",
    "        colormap=cmap\n",
    "    ).generate(joined_words)\n",
    "\n",
    "    ax.imshow(wcloud)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Wordcloud for {race}\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"./images/wordcloud.jpg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of the wordclouds\n",
    "\n",
    "![background](./images/bg.jpg)\n",
    "\n",
    "In the wordclouds it becomes apparent that the different races in Zelda has different words describing their actions. For instance we see that the people of Goron seem talkative and the people of Gerudo seem Sneaky. Looking at the wordcloud for Rito it would maybe also suggest, that they are an NPC giving quests to link, as some of the words from it are: inform, explain, remind, and prepare, which we intiuitively intepret as common words in video games when giving quests to the player. An example would be \"Name of someone from Rito\" informed/explained/reminded/prepared something to Link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Communities and TF-IDF\n",
    "## Explanation of the Louvain Algorithm\n",
    "The Louvain algorithm consists of two steps: Modularity Optimization and Community Aggregation. \n",
    "\n",
    "In the modularity optimization step, the algorithm randomly orders all nodes in the network. From here, it removes and inserts each node in different communities until no significant increase in modularity is seen.\n",
    "\n",
    "In the second step of community aggregation, the algorithm merges all nodes belonging to the same community into one node. This step can create self-loops. These loops are collapsed into the same giant nodes.\n",
    "\n",
    "## Choice of community algorithm\n",
    "For this exercise, we chose to use the Louvain algorithm to identify communities; as we from classes had an intuition about how it worked and had seen in previous exercises, its result seemed promising.\n",
    "\n",
    "## Preliminary code\n",
    "Below we create the communities and TF, IDF, and TF-IDF ratings for each community. First, we plot the community in nx to get a visual sense of the data. Then calculations,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming GCC to non-directed graph\n",
    "UGCC = GCC.to_undirected()\n",
    "\n",
    "# Using Louvain to find the best communities\n",
    "partition = community.best_partition(UGCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "pos = nx.spring_layout(UGCC)\n",
    "# color the nodes according to their partition\n",
    "# cmap = matplotlib.colors.get_cmap('viridis', max(partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(UGCC, pos, partition.keys(), node_size=40,\n",
    "                       cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(UGCC, pos, alpha=0.5)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('white')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Graph with community diferenciated with colors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for constructing dicts with information from the partition,\n",
    "# and for calculating modularity for a community given a graph\n",
    "\n",
    "def construct_communities(partition, G):\n",
    "  communities = dict()\n",
    "\n",
    "  for node_name in G.nodes():\n",
    "    key = partition[node_name]\n",
    "    communities[key] = communities.get(key, [])\n",
    "    communities[key].append(node_name)\n",
    "  \n",
    "  return communities\n",
    "\n",
    "def modularity(communities, graph):\n",
    "  modularity = 0\n",
    "  for com in communities:\n",
    "    Lc = len(graph.edges(communities[com]))\n",
    "    L = len(graph.edges)\n",
    "    kc = sum(x[1] for x in list(graph.degree(communities[com])))\n",
    "    \n",
    "    modularity += Lc/L - (kc/(2*L))**2\n",
    "  return modularity\n",
    "\n",
    "communities = construct_communities(partition, UGCC)\n",
    "mod = modularity(communities, UGCC)\n",
    "\n",
    "# Setting the dictionary keys to top3 names, as specified in exercise\n",
    "for key, com in dict(communities).items():\n",
    "  top3 = sorted([name for name in com], key = lambda x: UGCC.degree(x), reverse=True)[:3]\n",
    "  names = \"_\".join([x.replace(\" \", \"-\") for x in top3])\n",
    "\n",
    "  # Adding top3 as new name, and deleting previous key\n",
    "  communities[names] = com\n",
    "  del communities[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of communities and modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of communities:\", len(set(partition.values())))\n",
    "print(\"Modularity:\", mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of community sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins, bars = plt.hist([len(x) for x in communities.values()])\n",
    "\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= max(col)\n",
    "\n",
    "\n",
    "for c, p in zip(col, bars):\n",
    "  plt.setp(p, 'facecolor', cmap(c))\n",
    "\n",
    "for c, b in zip(counts, bins):\n",
    "    if c > 0: plt.text(b+4, c+.05, f\"{c:.0f}\")\n",
    "      \n",
    "plt.title(\"Histogram of the size of the communities\")\n",
    "plt.xlabel(\"Size of the community\")\n",
    "plt.ylabel(\"Ocurrences\")\n",
    "plt.show() # Fix colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises on the top 5 of communities\n",
    "\n",
    "### Methods for computing TF and IDF\n",
    "**TF** is calculated by the number of times a term is mentioned in a text and dividing it by the number of words. As this solution did not seem demanding, this solution is relatively trivial. Because of the need to multiply TF and IDF in TF-IDF, we put the values into a data frame, as vectorized calculations are way faster than conventional loops in python.\n",
    "\n",
    "**IDF** Is calculated using the formula $ln(\\frac{Num\\_documents}{Num\\_documents\\_word\\_in})$. Because this seemed a little more intensive, the counts are added to a data frame, and the logarithmic and division operation is done using vectorized calls with pandas.\n",
    "\n",
    "**TF-IDF** Is calculated using the formula $TF*IDF$. We multiply the two columns of the previous data frame **TF** and **IDF** to speed things up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TF, IDF and TF-IDF for communities\n",
    "\n",
    "sorted_communities = sorted(communities.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "largest = sorted_communities[:5]\n",
    "\n",
    "tfidf_coms = {}\n",
    "for (key, names) in largest:\n",
    "    filenames = [name + \".txt\" for name in names]\n",
    "    wlist = nltk.corpus.PlaintextCorpusReader(processed_path, filenames)\n",
    "    \n",
    "    terms = set(wlist.words())\n",
    "    text = nltk.Text(wlist.words())\n",
    "    \n",
    "    tfs = [[term, TF(term, text)] for term in terms]\n",
    "    tdf = pd.DataFrame(tfs, columns=[\"Term\", \"TF\"])\n",
    "    idf = IDF(terms, wlist) # Referencing a previous function in wordclouds\n",
    "\n",
    "    temp_df = pd.merge(tdf, idf, on=\"Term\")\n",
    "    temp_df[\"TFIDF\"] = temp_df.TF * temp_df.IDF\n",
    "\n",
    "    tfidf_coms[key] = temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 words for each community according to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 5 according to TF\")\n",
    "for com, frame in tfidf_coms.items():\n",
    "    print(\"Community:\", com)\n",
    "    display(frame.sort_values(by=\"TF\", ascending=False).head(5)[[\"Term\", \"TF\"]].style.hide_index())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 words for each community according to TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 5 according to TF-IDF\")\n",
    "for com, frame in tfidf_coms.items():\n",
    "    print(\"Community:\", com)\n",
    "    display(frame.sort_values(by=\"TFIDF\", ascending=False).head(5)[[\"Term\", \"TFIDF\"]].style.hide_index())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sentiment of communities\n",
    "First we do the sentiment analysis of characters. This is done in the LabMT and Vader section, named after the lexicons. Then we look at the histograms, happiest and saddest characters, and the community analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LabMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = json.loads(open(\"Dialogue/CharactersDialogue.json\", \"r\").read())\n",
    "pretty_dialogue = {}\n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "# We're not removing punctuation, as if the symbol, is not present\n",
    "# in LabMT, we will not score it.\n",
    "for name, word_list in dialogue.items():\n",
    "    lemmatized = wnl.lemmatize(\" \".join(word_list))\n",
    "    lower = lemmatized.lower()\n",
    "    tokens = wpt.tokenize(lower)\n",
    "    pretty_dialogue[name] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentimenting\n",
    "LabMT = pd.read_csv(\"./Wordlist/pone.0026752.s001.txt\", delimiter=\"\\t\", skiprows=3)\n",
    "LabMT_sentiments = {}\n",
    "\n",
    "lab_sentiments = {}\n",
    "for name, tokens in tqdm(pretty_dialogue.items()):\n",
    "    freq = nltk.FreqDist(tokens)\n",
    "    nominator, denominator = 0, 0\n",
    "\n",
    "    for word in freq.keys():\n",
    "        if word in LabMT.word.values:\n",
    "            fk = freq.get(word)\n",
    "            vk = LabMT[LabMT.word == word].happiness_average.values[0]\n",
    "\n",
    "            nominator += vk * fk\n",
    "            denominator += fk\n",
    "    \n",
    "    if denominator > 0:\n",
    "        lab_sentiments[name] = nominator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VADER = SentimentIntensityAnalyzer()\n",
    "VADER_sentiments = {}\n",
    "for name, tokens in tqdm(dialogue.items()):\n",
    "    VADER_sentiments[name] = VADER.polarity_scores(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of characters\n",
    "First we show the histograms, then the top 10 happiest and saddest according to LabMT and Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "fig, (lab, vad) = plt.subplots(1, 2, **default)\n",
    "\n",
    "counts, bins, bars = lab.hist(lab_sentiments.values())\n",
    "lab.set_title(\"Histogram of happiness obtained by LabMT\")\n",
    "lab.set_xlabel(\"Value\")\n",
    "lab.set_ylabel(\"Ocurrences\")\n",
    "\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= max(col)\n",
    "\n",
    "\n",
    "for c, p in zip(col, bars):\n",
    "  plt.setp(p, 'facecolor', cmap(c))\n",
    "\n",
    "counts, bins, bars = vad.hist([x[\"compound\"] for x in VADER_sentiments.values()])\n",
    "vad.set_title(\"Histogram of compound obtained by VADER\")\n",
    "vad.set_xlabel(\"Value\")\n",
    "vad.set_ylabel(\"Ocurrences\")\n",
    "\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= max(col)\n",
    "\n",
    "\n",
    "for c, p in zip(col, bars):\n",
    "  plt.setp(p, 'facecolor', cmap(c))\n",
    "\n",
    "  \n",
    "print(bins, bars)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10\n",
    "lab_sentiment_sorted = sorted(lab_sentiments.items(), key=lambda x: x[1], reverse=True)\n",
    "vader_compound_sorted = sorted(VADER_sentiments.items(), key=lambda x: x[1][\"compound\"], reverse=True)\n",
    "\n",
    "num = 10\n",
    "\n",
    "print(\"Happiest lab:\\n\", \"\\n \".join([str(x) + \": \" + str(y)\n",
    "                                     for x, y in lab_sentiment_sorted[:num]])) # Happiest lab\n",
    "print(\"\\nSaddest lab:\\n\", \"\\n \".join([str(x) + \": \" + str(y)\n",
    "                                      for x, y in lab_sentiment_sorted[-num:]])) # Saddest lab\n",
    "print(\"\\nHappiest vader:\\n\", \"\\n \".join([str(x) + \": \" + str(y[\"compound\"])\n",
    "                                         for x, y in vader_compound_sorted[:num]])) # \n",
    "print(\"\\nSaddest vader:\\n\", \"\\n \".join([str(x) + \": \" + str(y[\"compound\"])\n",
    "                                        for x, y in vader_compound_sorted[-num:]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average score of LabMT: lab_avg... and VADER: vad_avg...\n",
    "lab_avg_communities = {}\n",
    "for name, characters in communities.items():\n",
    "    lab_avg_communities[name] = sum([lab_sentiments[cname] \n",
    "                                    for cname in characters \n",
    "                                    if cname in lab_sentiments.keys()])/len(characters)\n",
    "                                    \n",
    "lab_avg_communities = {name: val for name, val in lab_avg_communities.items() if val != 0.0}\n",
    "\n",
    "vad_avg_communities = {}\n",
    "for name, characters in communities.items():\n",
    "    vad_avg_communities[name] = sum([VADER_sentiments[cname][\"compound\"] \n",
    "                                    for cname in characters \n",
    "                                    if cname in VADER_sentiments.keys()])/len(characters)\n",
    "\n",
    "vad_avg_communities = {name: val for name, val in vad_avg_communities.items() if val != 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Happiest and saddest communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Happiest\n",
    "print(\"Happiest communities LabMT\")\n",
    "print(\"\\n\".join([str(x) + \": \" + str(y) for x, y in sorted(lab_avg_communities.items(),\n",
    "                                                           key = lambda x: x[1],\n",
    "                                                           reverse=True)[:3]]))\n",
    "\n",
    "print()\n",
    "print(\"Happiest communities Vader\")\n",
    "print(\"\\n\".join([str(x) + \": \" + str(y) for x, y in sorted(vad_avg_communities.items(),\n",
    "                                                           key = lambda x: x[1],\n",
    "                                                           reverse=True)[:3]]))\n",
    "\n",
    "print()\n",
    "print(\"Saddest communities LabMT\")\n",
    "print(\"\\n\".join([str(x) + \": \" + str(y) for x, y in sorted(lab_avg_communities.items(),\n",
    "                                                           key = lambda x: x[1],\n",
    "                                                           reverse=True)[-3:]]))\n",
    "\n",
    "print()\n",
    "print(\"Saddest communities Vader\")\n",
    "print(\"\\n\".join([str(x) + \": \" + str(y) for x, y in sorted(vad_avg_communities.items(),\n",
    "                                                           key = lambda x: x[1],\n",
    "                                                           reverse=True)[-3:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_std = {}\n",
    "\n",
    "lab_std = {name: (sum([(lab_sentiments[cname] - lab_avg_communities[name])**2 \n",
    "                        for cname in characters \n",
    "                        if cname in lab_sentiments.keys()])/len(characters))**.5\n",
    "                        for name, characters in communities.items()\n",
    "                        if name in lab_avg_communities.keys()}\n",
    "                        \n",
    "vad_std = {name: (sum([(VADER_sentiments[cname][\"compound\"] - vad_avg_communities[name])**2 \n",
    "                        for cname in characters \n",
    "                        if cname in VADER_sentiments.keys()])/len(characters))**.5\n",
    "                        for name, characters in communities.items()\n",
    "                        if name in vad_avg_communities.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (lab, vad) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "bars = lab.bar(lab_avg_communities.keys(), lab_avg_communities.values(), yerr=lab_std.values())\n",
    "\n",
    "for tick in lab.get_xticklabels():\n",
    "    tick.set_rotation(90)\n",
    "lab.set_title(\"Average happiness value by community calculated with LabMT\")\n",
    "lab.set_xlabel(\"Community\")\n",
    "lab.set_ylabel(\"Value\")\n",
    "\n",
    "for c, p in zip(range(len(bars)), bars):\n",
    "  plt.setp(p, 'facecolor', cmap(c%cmap.N))\n",
    "\n",
    "bars = vad.bar(vad_avg_communities.keys(), vad_avg_communities.values(), yerr=vad_std.values())\n",
    "for tick in vad.get_xticklabels():\n",
    "    tick.set_rotation(90)\n",
    "vad.set_title(\"Average compound value by community calculated with VADER\")\n",
    "vad.set_xlabel(\"Community\")\n",
    "vad.set_ylabel(\"Value\")\n",
    "\n",
    "for c, p in zip(range(len(bars)), bars):\n",
    "  plt.setp(p, 'facecolor', cmap(c%cmap.N))\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences between LabMT and VADER\n",
    "\n",
    "The main difference etween the two ways of obtaining text sentiments is that LabMT is a dictionary-based approach, while VADER is a rule-based approach.\n",
    "\n",
    "What can be seen in the previous plots is that both methods analyse the texts in a similar way. If one community has a greater value in LabMT, it will probably have a bigger value in VADER, and viceversa. However, VADER sometimes has a few differneces, probably due the fact that uses the context, so probably in those cases the text had negatives or sentences that changed the value of the sentiment.\n",
    "\n",
    "Another striking fact is that the `std` seems to be greater in VADER, and can be explained againd by the fact that it uses context, so probably more texts inside a community have different sentiment, and is able to display it properly, not like LabMT.\n",
    "\n",
    "\n",
    "## Advantage of rule-based method\n",
    "\n",
    "The main advantage in a rule based method, over discyionary-based approachs is that it, at least, makes an attemp on understanding the context, while the dictionary-based only look up the vale of each word in a table.\n",
    "\n",
    "Using different rules it can change the value and sentiment of a sentence, or taking into account negatives or exclamation marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ending\n",
    "Welcome to the other side, we hope you enjoyed your stay ðŸ˜"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8199bc9c93b4262823634d25f4a32e04dd527ed93790c91211f95e4ad523798"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
